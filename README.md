# GPT-2 Language Model from Scratch

This repository implements a basic version of a GPT-like language model, following a tutorial inspired by Andrej Karpathy’s [Zero to Hero](https://karpathy.ai/zero-to-hero.html) course on GPT.

## Overview

In this project, we build a character-level language model using PyTorch. The model is trained on the tiny Shakespeare dataset and learns to predict the next character in a sequence of text. It is a good introduction to understanding how transformers and self-attention work under the hood.

The notebook demonstrates:

- Data preparation and tokenization.
- Basic training loop for a language model.
- Model architecture and attention mechanism.
- Generation of new text sequences based on a trained model.

## Requirements

- Python 3.x
- PyTorch
- NumPy
- Other dependencies listed in `requirements.txt` (if available).

## Dataset

We use the [Tiny Shakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) dataset, which is a collection of text from Shakespeare’s plays. It’s a small but effective dataset for training simple character-level language models.

## Model Architecture

This project implements a simplified version of the GPT architecture. The model consists of:

- **Embedding Layer**: Each token is mapped to a high-dimensional vector representation.
- **Self-Attention Layer**: The model attends to different parts of the input sequence to capture contextual relationships between characters.
- **Feedforward Neural Network**: Applied after attention to process the information.
- **Output Layer**: Produces the logits for the next token in the sequence.

The model is trained using a cross-entropy loss function and an AdamW optimizer.

## How to Run

1. Clone this repository:

   ```bash
   git clone https://github.com/yourusername/gpt-from-scratch.git
   cd gpt-from-scratch````


2. Install the necessary dependencies:

   ```bash
   pip install -r requirements.txt
   ```

3. Run the Jupyter notebook:

   ```bash
   jupyter notebook
   ```

4. Follow the steps in the notebook to load the dataset, train the model, and generate new text.

## Training Process

The training loop involves the following steps:

1. **Batch Generation**: Small batches of input sequences (`x`) and targets (`y`) are generated.
2. **Forward Pass**: The model processes the input and computes the loss.
3. **Backpropagation**: The loss is backpropagated, and the optimizer updates the model parameters.

The loss is evaluated periodically on both training and validation sets.

## Text Generation

Once trained, the model can generate new text by sampling the most likely next characters given an initial context. The `generate()` function uses the model to predict the next tokens iteratively.

## Example Generation

After training the model, you can generate new text sequences like so:

```python
context = torch.zeros((1, 1), dtype=torch.long, device=device)
print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))
```

## Model Evaluation

The model’s performance is evaluated based on the loss on the validation set. Training and validation losses are printed after each evaluation step.

## Notes

* **Self-attention**: The core idea behind self-attention is that every token in the input sequence can "attend" to every other token. The attention mechanism computes a weighted sum of other tokens, with weights determined by learned parameters.
* **Layer Normalization**: Helps stabilize the training of deep networks.
* **Text Generation**: New text is generated by predicting one character at a time and feeding the predicted character back as input.



```

```
